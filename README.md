# Japanese LLM Benchmark Suite

A lightweight benchmarking suite for Japanese-language large-scale models.  
Focus: **inference speed, memory efficiency, and generation quality** across multiple deployment methods.

## Objectives

- Compare performance of Japanese-specific models (`rinna`, `cl-tohoku`, etc.)
- Evaluate quantized variants (QLoRA, GPTQ)
- Analyze tradeoffs: speed vs accuracy vs memory
- Build a reproducible benchmark on minimal hardware (MacBook M1+)

## Models Covered

| Model                        | Type           | Tokenizer         |
|-----------------------------|----------------|-------------------|
| rinna/japanese-gpt2-medium  | GPT-style      | SentencePiece     |
| cl-tohoku/bert-base-japanese-v2 | BERT      | WordPiece         |
| distil-japanese-BERT        | Distilled BERT | WordPiece         |

## â›® Inference Backends

- Transformers (CPU/GPU)
- ONNX Runtime
- GGUF / llama.cpp (planned)

## Benchmark Metrics

- Tokens/sec (inference throughput)
- Memory usage (resident size, max alloc)
- Quality scores (BLEU, ROUGE, JL-BERTScore)

## Dataset Sources

- é’ç©ºæ–‡åº« (Aozora Bunko)
- livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹
- æ—¥æœ¬èª Wikipedia æŠœç²‹

## Results Preview

Coming soon â€” benchmark tables for:
- QLoRA vs GPTQ performance on `rinna` models
- SentencePiece vs WordPiece tokenization drift
- ONNX vs Transformers latency on CPU

## Blog + Results

- [Blog 1: ã€ŒLLMæ¨è«–ã‚’Macã§å§‹ã‚ã‚‹æ–¹æ³•ã€ (äºˆå®š)](link)
- [Blog 2: ã€Œæ—¥æœ¬èªLLMã®é‡å­åŒ–æ¯”è¼ƒï¼šQLoRA vs GPTQã€ (äºˆå®š)](link)

## ğŸ“„ License

MIT. See `LICENSE.md`.
